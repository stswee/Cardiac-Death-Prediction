{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3b6e80-e53a-4f4c-837a-df48d4a7f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPUs\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d01b8c-60a6-4fb1-bdc1-a4689b75dbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prognosis</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RANKING: 1. Survivor\\nREASONING: \\n\\nBased on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RANKING: 1. Survivor\\nREASONING:\\n\\nBased on t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RANKING: 1. Survivor\\nREASONING: The patient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RANKING: \\n1. Survivor\\nREASONING:\\nBased on t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RANKING: 1. Survivor\\nREASONING: \\nBased on th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Prognosis  labels\n",
       "0  RANKING: 1. Survivor\\nREASONING: \\n\\nBased on ...       0\n",
       "1  RANKING: 1. Survivor\\nREASONING:\\n\\nBased on t...       0\n",
       "2  RANKING: 1. Survivor\\nREASONING: The patient i...       0\n",
       "3  RANKING: \\n1. Survivor\\nREASONING:\\nBased on t...       0\n",
       "4  RANKING: 1. Survivor\\nREASONING: \\nBased on th...       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Loading\n",
    "\n",
    "# Import packages\n",
    "import pandas as pd\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv(\"../Data/subject-info-cleaned-with-prognosis-D.csv\")\n",
    "\n",
    "# Get prognosis and outcome\n",
    "df = df[['Prognosis', 'Outcome']]\n",
    "\n",
    "# Map labels to integers\n",
    "# Survivor = 0, SCD = 1, PFD = 2\n",
    "label_map = {\"survivor\": 0, \"sudden cardiac death\": 1, \"pump failure death\": 2}\n",
    "df['Outcome'] = df['Outcome'].map(label_map)\n",
    "df.rename(columns = {\"Outcome\": \"labels\"}, inplace = True)\n",
    "df.head()\n",
    "\n",
    "# Shuffle dataset\n",
    "# df = df.sample(frac = 1, random_state = 42)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df8a3c57-95cd-49e1-97e3-d11ec10f0763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANKING: \n",
      "1. Pump failure death\n",
      "2. Sudden cardiac death\n",
      "3. Survivor\n",
      "\n",
      "REASONING:\n",
      "\n",
      "Based on the provided patient data, I would rank the prognoses from most likely to least likely as follows:\n",
      "\n",
      "Pump failure death is the most likely outcome in this patient. The patient has a history of Hypertrophic cardiomyopathy, which is a condition that can lead to left ventricular dysfunction and heart failure. The patient's LVEF of 40.0 indicates significant left ventricular dysfunction, which is a strong predictor of heart failure. Additionally, the patient's elevated creatinine level (115.0 mmol/L) suggests impaired renal function, which is often associated with heart failure. The presence of ventricular extrasystoles on the ECG, which is a marker of ventricular instability, further supports this prediction.\n",
      "\n",
      "Sudden cardiac death is the second most likely outcome. The patient's ECG shows polymorphic ventricular extrasystoles, which can be a marker of ventricular instability and an increased risk of sudden cardiac death. The patient's NYHA Class II status also indicates some limitation in physical activity, which can increase the risk of sudden cardiac death.\n",
      "\n",
      "Survivor is the least likely outcome. While the patient has a history of hypertension and Hypertrophic cardiomyopathy, the patient's current LVEF of 40.0 is relatively low, and the presence of ventricular extrasystoles on the ECG suggests a higher risk of adverse outcomes. Additionally, the patient's elevated creatinine level and impaired renal function may indicate underlying kidney disease, which can further complicate the patient's prognosis.\n",
      "\n",
      "Overall, the combination of left ventricular dysfunction, ventricular instability, and impaired renal function make pump failure death the most likely outcome in this patient.\n"
     ]
    }
   ],
   "source": [
    "print(df['Prognosis'][810])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae4da5e5-8eda-49fe-849f-fe8e82a2c108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sswee/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in BioBERT with classification head\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch import cuda\n",
    "\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "num_labels = 3  # Three possible outcomes\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a65164ec-f466-4290-b0d5-5e57f46b006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Data split (for testing)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size = 0.3, random_state = 42, stratify = df['labels'])\n",
    "\n",
    "# Get encodings\n",
    "train_encodings = tokenizer(list(train_df['Prognosis']), truncation = True, padding = True)\n",
    "test_encodings = tokenizer(list(test_df['Prognosis']), truncation = True, padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d0eee36-8788-40a0-b15a-58a6e3a63c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ac4529-9fc0-4360-bc59-ce82385216f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 849/849 [00:00<00:00, 1671.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load BioBERT tokenizer\n",
    "# model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "num_labels = 3  # Three possible outcomes\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Prognosis\"], padding=\"max_length\", truncation=True, max_length = 512)\n",
    "\n",
    "# Apply tokenization\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a293c0b1-3d7f-4af2-b9bb-5a63047a5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_test = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test[\"train\"]\n",
    "val_dataset = train_test[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f00b7f2d-a0c5-4c16-b5a0-f63e4e6d195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)  # Convert logits to class predictions\n",
    "\n",
    "    # Compute accuracy\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    # Compute precision, recall, f1-score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    # Compute AUC (only if there are at least 2 classes)\n",
    "    # auc = roc_auc_score(labels, logits, multi_class=\"ovr\") if len(set(labels)) > 1 else 0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "        #\"auc\": auc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2c793d1-212d-4e51-8c99-22ae1bbc8d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sswee/myenv/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../Results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"../logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a921c24e-09e4-4631-81f8-349b4e916903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4061917/360158454.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/home/sswee/myenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [129/129 01:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.485500</td>\n",
       "      <td>0.391664</td>\n",
       "      <td>0.888235</td>\n",
       "      <td>0.896955</td>\n",
       "      <td>0.888235</td>\n",
       "      <td>0.872773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.451000</td>\n",
       "      <td>0.379228</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.760660</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.800151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.343995</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.904046</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.898669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sswee/myenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/sswee/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sswee/myenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/sswee/myenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=129, training_loss=0.41489963198817054, metrics={'train_runtime': 81.5779, 'train_samples_per_second': 24.97, 'train_steps_per_second': 1.581, 'total_flos': 535962031911936.0, 'train_loss': 0.41489963198817054, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b747f67c-9573-4a25-bb44-544348a7def4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sswee/myenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.34399500489234924,\n",
       " 'eval_accuracy': 0.9058823529411765,\n",
       " 'eval_precision': 0.9040459467369037,\n",
       " 'eval_recall': 0.9058823529411765,\n",
       " 'eval_f1': 0.8986689270302716,\n",
       " 'eval_runtime': 0.9122,\n",
       " 'eval_samples_per_second': 186.362,\n",
       " 'eval_steps_per_second': 12.059,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
